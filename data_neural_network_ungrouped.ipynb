{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('full_data6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['won'] = 0\n",
    "df.loc[df.place==1, 'won'] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['race_id', 'horse_id', 'horse_age', 'horse_weight', 'horse_win_percent',\n",
       "       'jockey_win_percent', 'trainer_win_percent', 'days_since_last_race',\n",
       "       'mean_speed_figure', 'last_figure', 'best_figure_dist',\n",
       "       'best_figure_going', 'top_speed', 'rating', 'odds', 'mean_rating',\n",
       "       'last_rating', 'last_official_rating', 'difference_in_rating',\n",
       "       'best_rating_going', 'best_official_rating_going', 'win_percent_going',\n",
       "       'best_rating_dist', 'best_official_rating_dist', 'win_percent_dist',\n",
       "       'placed', 'length', 'sire_win_percent', 'dam_win_percent',\n",
       "       'dam_sire_win_percent', 'sire_og_going_win_percent',\n",
       "       'sire_prog_going_win_percent', 'sire_og_type_win_percent',\n",
       "       'sire_prog_type_win_percent', 'sire_og_dist_win_percent',\n",
       "       'sire_prog_dist_win_percent', 'dam_og_going_win_percent',\n",
       "       'dam_prog_going_win_percent', 'dam_og_type_win_percent',\n",
       "       'dam_prog_type_win_percent', 'dam_og_dist_win_percent',\n",
       "       'dam_prog_dist_win_percent', 'place', 'won'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horse_age</th>\n",
       "      <th>horse_weight</th>\n",
       "      <th>horse_win_percent</th>\n",
       "      <th>jockey_win_percent</th>\n",
       "      <th>trainer_win_percent</th>\n",
       "      <th>days_since_last_race</th>\n",
       "      <th>mean_speed_figure</th>\n",
       "      <th>last_figure</th>\n",
       "      <th>best_figure_dist</th>\n",
       "      <th>best_figure_going</th>\n",
       "      <th>...</th>\n",
       "      <th>sire_prog_type_win_percent</th>\n",
       "      <th>sire_og_dist_win_percent</th>\n",
       "      <th>sire_prog_dist_win_percent</th>\n",
       "      <th>dam_og_going_win_percent</th>\n",
       "      <th>dam_prog_going_win_percent</th>\n",
       "      <th>dam_og_type_win_percent</th>\n",
       "      <th>dam_prog_type_win_percent</th>\n",
       "      <th>dam_og_dist_win_percent</th>\n",
       "      <th>dam_prog_dist_win_percent</th>\n",
       "      <th>won</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.40825</td>\n",
       "      <td>-0.71306</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.14040</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.37207</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.01029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.14040</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.37207</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.44949</td>\n",
       "      <td>2.17670</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.29305</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03936</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.90248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.29305</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03936</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.40825</td>\n",
       "      <td>0.07506</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.11828</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.20049</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.11828</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.20049</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.40825</td>\n",
       "      <td>0.07506</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.20388</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.64023</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.38914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.20388</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.64023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.40825</td>\n",
       "      <td>0.07506</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.12064</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.26074</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.01104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.12064</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.26074</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751794</th>\n",
       "      <td>-0.18898</td>\n",
       "      <td>-0.08234</td>\n",
       "      <td>2.14497</td>\n",
       "      <td>-1.56893</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>-0.88513</td>\n",
       "      <td>0.97156</td>\n",
       "      <td>1.03023</td>\n",
       "      <td>1.06890</td>\n",
       "      <td>0.19654</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.22265</td>\n",
       "      <td>-0.35355</td>\n",
       "      <td>-0.27316</td>\n",
       "      <td>0.27612</td>\n",
       "      <td>-0.42540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.22265</td>\n",
       "      <td>-0.35355</td>\n",
       "      <td>-0.27316</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751795</th>\n",
       "      <td>-0.18898</td>\n",
       "      <td>-0.08234</td>\n",
       "      <td>-0.80343</td>\n",
       "      <td>0.19612</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.74517</td>\n",
       "      <td>-0.55956</td>\n",
       "      <td>0.80004</td>\n",
       "      <td>-0.24776</td>\n",
       "      <td>-0.20710</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.66810</td>\n",
       "      <td>-0.35355</td>\n",
       "      <td>-0.59285</td>\n",
       "      <td>-1.67974</td>\n",
       "      <td>-1.47400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.66810</td>\n",
       "      <td>-0.35355</td>\n",
       "      <td>-0.59285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751796</th>\n",
       "      <td>-0.75593</td>\n",
       "      <td>-0.08234</td>\n",
       "      <td>-0.80343</td>\n",
       "      <td>-0.68641</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>1.35413</td>\n",
       "      <td>0.38907</td>\n",
       "      <td>-0.55556</td>\n",
       "      <td>0.51076</td>\n",
       "      <td>-0.04564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26534</td>\n",
       "      <td>-0.35355</td>\n",
       "      <td>-0.11210</td>\n",
       "      <td>0.27612</td>\n",
       "      <td>-0.01634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.26534</td>\n",
       "      <td>-0.35355</td>\n",
       "      <td>-0.11210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751797</th>\n",
       "      <td>-1.32288</td>\n",
       "      <td>-1.56444</td>\n",
       "      <td>-0.80343</td>\n",
       "      <td>1.96116</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>0.07775</td>\n",
       "      <td>-1.92810</td>\n",
       "      <td>-1.61389</td>\n",
       "      <td>-2.39155</td>\n",
       "      <td>-2.30602</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.83710</td>\n",
       "      <td>2.82843</td>\n",
       "      <td>-1.23733</td>\n",
       "      <td>1.70279</td>\n",
       "      <td>0.15175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.83710</td>\n",
       "      <td>2.82843</td>\n",
       "      <td>-1.23733</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751798</th>\n",
       "      <td>-1.32288</td>\n",
       "      <td>-0.08234</td>\n",
       "      <td>0.60070</td>\n",
       "      <td>0.19612</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.35413</td>\n",
       "      <td>-0.15006</td>\n",
       "      <td>0.03582</td>\n",
       "      <td>-0.24776</td>\n",
       "      <td>-0.32463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.66810</td>\n",
       "      <td>-0.35355</td>\n",
       "      <td>-0.59285</td>\n",
       "      <td>-1.67974</td>\n",
       "      <td>-1.47400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.66810</td>\n",
       "      <td>-0.35355</td>\n",
       "      <td>-0.59285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1751799 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         horse_age  horse_weight  horse_win_percent  jockey_win_percent  \\\n",
       "0         -0.40825      -0.71306            0.00000             0.00000   \n",
       "1          2.44949       2.17670            0.00000             0.00000   \n",
       "2         -0.40825       0.07506            0.00000             0.00000   \n",
       "3         -0.40825       0.07506            0.00000             0.00000   \n",
       "4         -0.40825       0.07506            0.00000             0.00000   \n",
       "...            ...           ...                ...                 ...   \n",
       "1751794   -0.18898      -0.08234            2.14497            -1.56893   \n",
       "1751795   -0.18898      -0.08234           -0.80343             0.19612   \n",
       "1751796   -0.75593      -0.08234           -0.80343            -0.68641   \n",
       "1751797   -1.32288      -1.56444           -0.80343             1.96116   \n",
       "1751798   -1.32288      -0.08234            0.60070             0.19612   \n",
       "\n",
       "         trainer_win_percent  days_since_last_race  mean_speed_figure  \\\n",
       "0                       0.00               0.00000            0.00000   \n",
       "1                       0.00               0.00000            0.00000   \n",
       "2                       0.00               0.00000            0.00000   \n",
       "3                       0.00               0.00000            0.00000   \n",
       "4                       0.00               0.00000            0.00000   \n",
       "...                      ...                   ...                ...   \n",
       "1751794                -1.25              -0.88513            0.97156   \n",
       "1751795                 1.00              -0.74517           -0.55956   \n",
       "1751796                -0.50               1.35413            0.38907   \n",
       "1751797                -1.25               0.07775           -1.92810   \n",
       "1751798                 1.75               1.35413           -0.15006   \n",
       "\n",
       "         last_figure  best_figure_dist  best_figure_going  ...  \\\n",
       "0            0.00000           0.00000            0.00000  ...   \n",
       "1            0.00000           0.00000            0.00000  ...   \n",
       "2            0.00000           0.00000            0.00000  ...   \n",
       "3            0.00000           0.00000            0.00000  ...   \n",
       "4            0.00000           0.00000            0.00000  ...   \n",
       "...              ...               ...                ...  ...   \n",
       "1751794      1.03023           1.06890            0.19654  ...   \n",
       "1751795      0.80004          -0.24776           -0.20710  ...   \n",
       "1751796     -0.55556           0.51076           -0.04564  ...   \n",
       "1751797     -1.61389          -2.39155           -2.30602  ...   \n",
       "1751798      0.03582          -0.24776           -0.32463  ...   \n",
       "\n",
       "         sire_prog_type_win_percent  sire_og_dist_win_percent  \\\n",
       "0                           1.14040                   0.00000   \n",
       "1                          -0.29305                   0.00000   \n",
       "2                          -1.11828                   0.00000   \n",
       "3                           1.20388                   0.00000   \n",
       "4                          -0.12064                   0.00000   \n",
       "...                             ...                       ...   \n",
       "1751794                    -0.22265                  -0.35355   \n",
       "1751795                    -0.66810                  -0.35355   \n",
       "1751796                     0.26534                  -0.35355   \n",
       "1751797                    -0.83710                   2.82843   \n",
       "1751798                    -0.66810                  -0.35355   \n",
       "\n",
       "         sire_prog_dist_win_percent  dam_og_going_win_percent  \\\n",
       "0                           0.37207                   0.00000   \n",
       "1                           0.03936                   0.00000   \n",
       "2                          -0.20049                   0.00000   \n",
       "3                           1.64023                   0.00000   \n",
       "4                          -0.26074                   0.00000   \n",
       "...                             ...                       ...   \n",
       "1751794                    -0.27316                   0.27612   \n",
       "1751795                    -0.59285                  -1.67974   \n",
       "1751796                    -0.11210                   0.27612   \n",
       "1751797                    -1.23733                   1.70279   \n",
       "1751798                    -0.59285                  -1.67974   \n",
       "\n",
       "         dam_prog_going_win_percent  dam_og_type_win_percent  \\\n",
       "0                          -0.01029                      0.0   \n",
       "1                          -0.90248                      0.0   \n",
       "2                           0.57081                      0.0   \n",
       "3                           1.38914                      0.0   \n",
       "4                           1.01104                      0.0   \n",
       "...                             ...                      ...   \n",
       "1751794                    -0.42540                      0.0   \n",
       "1751795                    -1.47400                      0.0   \n",
       "1751796                    -0.01634                      0.0   \n",
       "1751797                     0.15175                      0.0   \n",
       "1751798                    -1.47400                      0.0   \n",
       "\n",
       "         dam_prog_type_win_percent  dam_og_dist_win_percent  \\\n",
       "0                          1.14040                  0.00000   \n",
       "1                         -0.29305                  0.00000   \n",
       "2                         -1.11828                  0.00000   \n",
       "3                          1.20388                  0.00000   \n",
       "4                         -0.12064                  0.00000   \n",
       "...                            ...                      ...   \n",
       "1751794                   -0.22265                 -0.35355   \n",
       "1751795                   -0.66810                 -0.35355   \n",
       "1751796                    0.26534                 -0.35355   \n",
       "1751797                   -0.83710                  2.82843   \n",
       "1751798                   -0.66810                 -0.35355   \n",
       "\n",
       "         dam_prog_dist_win_percent  won  \n",
       "0                          0.37207    0  \n",
       "1                          0.03936    0  \n",
       "2                         -0.20049    1  \n",
       "3                          1.64023    0  \n",
       "4                         -0.26074    0  \n",
       "...                            ...  ...  \n",
       "1751794                   -0.27316    0  \n",
       "1751795                   -0.59285    0  \n",
       "1751796                   -0.11210    0  \n",
       "1751797                   -1.23733    0  \n",
       "1751798                   -0.59285    0  \n",
       "\n",
       "[1751799 rows x 37 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_df = df.drop(['horse_id','race_id','top_speed','rating','place','placed','length'],axis=1)\n",
    "current_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1          0\n",
       "2          1\n",
       "3          0\n",
       "4          0\n",
       "          ..\n",
       "1751794    0\n",
       "1751795    0\n",
       "1751796    0\n",
       "1751797    0\n",
       "1751798    0\n",
       "Name: won, Length: 1751799, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = current_df.drop('won',axis=1)\n",
    "y = current_df['won']\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=1)\n",
    "X_test, X_validate, y_test, y_valiate = model_selection.train_test_split(X_test, y_test, train_size=0.8, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leish\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(180, activation='relu', input_shape=(36,)),\n",
    "    #tf.keras.layers.ReLU(max_value=None, negative_slope=0, threshold=0),\n",
    "    #tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'),\n",
    "    #tf.keras.layers.Dropout(.2),\n",
    "    tf.keras.layers.Dense(180, activation='relu'),\n",
    "    #tf.keras.layers.ReLU(max_value=None, negative_slope=0, threshold=0),\n",
    "    #tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'),   \n",
    "    #tf.keras.layers.Dropout(.2),\n",
    "    tf.keras.layers.Dense(180, activation='relu'),\n",
    "    #tf.keras.layers.ReLU(max_value=None, negative_slope=0, threshold=0),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\"\"\"model.compile(optimizer=tf.keras.optimizers.Adam(5e-04),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.Accuracy(name='accuracy')])\"\"\"\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.001),\n",
    "              loss=tf.keras.losses.binary_crossentropy,\n",
    "              metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training..\n",
      "\n",
      "Epoch 1/250\n",
      "2803/2803 [==============================] - 35s 11ms/step - loss: 0.3685 - binary_accuracy: 0.8881 - val_loss: 0.3264 - val_binary_accuracy: 0.8919\n",
      "Epoch 2/250\n",
      "2803/2803 [==============================] - 31s 10ms/step - loss: 0.3185 - binary_accuracy: 0.8921 - val_loss: 0.3130 - val_binary_accuracy: 0.8919\n",
      "Epoch 3/250\n",
      "2803/2803 [==============================] - 34s 12ms/step - loss: 0.3090 - binary_accuracy: 0.8921 - val_loss: 0.3060 - val_binary_accuracy: 0.8919\n",
      "Epoch 4/250\n",
      "2803/2803 [==============================] - 43s 15ms/step - loss: 0.3035 - binary_accuracy: 0.8921 - val_loss: 0.3015 - val_binary_accuracy: 0.8920\n",
      "Epoch 5/250\n",
      "2803/2803 [==============================] - 48s 16ms/step - loss: 0.2998 - binary_accuracy: 0.8923 - val_loss: 0.2984 - val_binary_accuracy: 0.8924\n",
      "Epoch 6/250\n",
      "2803/2803 [==============================] - 55s 19ms/step - loss: 0.2972 - binary_accuracy: 0.8927 - val_loss: 0.2962 - val_binary_accuracy: 0.8929\n",
      "Epoch 7/250\n",
      "2803/2803 [==============================] - 55s 19ms/step - loss: 0.2953 - binary_accuracy: 0.8930 - val_loss: 0.2945 - val_binary_accuracy: 0.8931\n",
      "Epoch 8/250\n",
      "2803/2803 [==============================] - 42s 14ms/step - loss: 0.2938 - binary_accuracy: 0.8932 - val_loss: 0.2932 - val_binary_accuracy: 0.8933\n",
      "Epoch 9/250\n",
      "2803/2803 [==============================] - 49s 17ms/step - loss: 0.2927 - binary_accuracy: 0.8933 - val_loss: 0.2922 - val_binary_accuracy: 0.8934\n",
      "Epoch 10/250\n",
      "2803/2803 [==============================] - 45s 15ms/step - loss: 0.2918 - binary_accuracy: 0.8934 - val_loss: 0.2913 - val_binary_accuracy: 0.8935\n",
      "Epoch 11/250\n",
      "2803/2803 [==============================] - 48s 16ms/step - loss: 0.2910 - binary_accuracy: 0.8935 - val_loss: 0.2906 - val_binary_accuracy: 0.8936\n",
      "Epoch 12/250\n",
      "2803/2803 [==============================] - 42s 14ms/step - loss: 0.2903 - binary_accuracy: 0.8935 - val_loss: 0.2900 - val_binary_accuracy: 0.8938\n",
      "Epoch 13/250\n",
      "2803/2803 [==============================] - 48s 16ms/step - loss: 0.2898 - binary_accuracy: 0.8936 - val_loss: 0.2895 - val_binary_accuracy: 0.8939\n",
      "Epoch 14/250\n",
      "2803/2803 [==============================] - 39s 13ms/step - loss: 0.2892 - binary_accuracy: 0.8936 - val_loss: 0.2890 - val_binary_accuracy: 0.8939\n",
      "Epoch 15/250\n",
      "2803/2803 [==============================] - 46s 16ms/step - loss: 0.2888 - binary_accuracy: 0.8937 - val_loss: 0.2886 - val_binary_accuracy: 0.8939\n",
      "Epoch 16/250\n",
      "2803/2803 [==============================] - 45s 15ms/step - loss: 0.2884 - binary_accuracy: 0.8937 - val_loss: 0.2882 - val_binary_accuracy: 0.8939\n",
      "Epoch 17/250\n",
      "2803/2803 [==============================] - 51s 17ms/step - loss: 0.2880 - binary_accuracy: 0.8937 - val_loss: 0.2879 - val_binary_accuracy: 0.8940\n",
      "Epoch 18/250\n",
      "2803/2803 [==============================] - 45s 16ms/step - loss: 0.2876 - binary_accuracy: 0.8938 - val_loss: 0.2875 - val_binary_accuracy: 0.8940\n",
      "Epoch 19/250\n",
      "2803/2803 [==============================] - 43s 14ms/step - loss: 0.2873 - binary_accuracy: 0.8938 - val_loss: 0.2872 - val_binary_accuracy: 0.8940\n",
      "Epoch 20/250\n",
      "2803/2803 [==============================] - 45s 15ms/step - loss: 0.2870 - binary_accuracy: 0.8938 - val_loss: 0.2870 - val_binary_accuracy: 0.8940\n",
      "Epoch 21/250\n",
      "2803/2803 [==============================] - 36s 12ms/step - loss: 0.2868 - binary_accuracy: 0.8938 - val_loss: 0.2867 - val_binary_accuracy: 0.8940\n",
      "Epoch 22/250\n",
      "2803/2803 [==============================] - 41s 14ms/step - loss: 0.2865 - binary_accuracy: 0.8939 - val_loss: 0.2865 - val_binary_accuracy: 0.8940\n",
      "Epoch 23/250\n",
      "2803/2803 [==============================] - 38s 13ms/step - loss: 0.2863 - binary_accuracy: 0.8939 - val_loss: 0.2863 - val_binary_accuracy: 0.8940\n",
      "Epoch 24/250\n",
      "2803/2803 [==============================] - 45s 15ms/step - loss: 0.2860 - binary_accuracy: 0.8939 - val_loss: 0.2860 - val_binary_accuracy: 0.8939\n",
      "Epoch 25/250\n",
      "2803/2803 [==============================] - 43s 15ms/step - loss: 0.2858 - binary_accuracy: 0.8939 - val_loss: 0.2858 - val_binary_accuracy: 0.8939\n",
      "Epoch 26/250\n",
      "2803/2803 [==============================] - 46s 16ms/step - loss: 0.2856 - binary_accuracy: 0.8939 - val_loss: 0.2857 - val_binary_accuracy: 0.8940\n",
      "Epoch 27/250\n",
      "2803/2803 [==============================] - 42s 14ms/step - loss: 0.2855 - binary_accuracy: 0.8939 - val_loss: 0.2855 - val_binary_accuracy: 0.8939\n",
      "Epoch 28/250\n",
      "2803/2803 [==============================] - 46s 16ms/step - loss: 0.2853 - binary_accuracy: 0.8939 - val_loss: 0.2853 - val_binary_accuracy: 0.8940\n",
      "Epoch 29/250\n",
      "2803/2803 [==============================] - 41s 14ms/step - loss: 0.2851 - binary_accuracy: 0.8940 - val_loss: 0.2852 - val_binary_accuracy: 0.8941\n",
      "Epoch 30/250\n",
      "2803/2803 [==============================] - 43s 15ms/step - loss: 0.2850 - binary_accuracy: 0.8940 - val_loss: 0.2850 - val_binary_accuracy: 0.8940\n",
      "Epoch 31/250\n",
      "2803/2803 [==============================] - 36s 12ms/step - loss: 0.2848 - binary_accuracy: 0.8940 - val_loss: 0.2849 - val_binary_accuracy: 0.8940\n",
      "Epoch 32/250\n",
      "2803/2803 [==============================] - 41s 14ms/step - loss: 0.2847 - binary_accuracy: 0.8940 - val_loss: 0.2848 - val_binary_accuracy: 0.8941\n",
      "Epoch 33/250\n",
      "2803/2803 [==============================] - 39s 13ms/step - loss: 0.2846 - binary_accuracy: 0.8940 - val_loss: 0.2846 - val_binary_accuracy: 0.8941\n",
      "Epoch 34/250\n",
      "2803/2803 [==============================] - 47s 16ms/step - loss: 0.2844 - binary_accuracy: 0.8940 - val_loss: 0.2845 - val_binary_accuracy: 0.8941\n",
      "Epoch 35/250\n",
      "2803/2803 [==============================] - 42s 14ms/step - loss: 0.2843 - binary_accuracy: 0.8940 - val_loss: 0.2844 - val_binary_accuracy: 0.8941\n",
      "Epoch 36/250\n",
      "2803/2803 [==============================] - 46s 16ms/step - loss: 0.2842 - binary_accuracy: 0.8940 - val_loss: 0.2843 - val_binary_accuracy: 0.8941\n",
      "Epoch 37/250\n",
      "2803/2803 [==============================] - 40s 14ms/step - loss: 0.2841 - binary_accuracy: 0.8940 - val_loss: 0.2842 - val_binary_accuracy: 0.8941\n",
      "Epoch 38/250\n",
      "2803/2803 [==============================] - 43s 15ms/step - loss: 0.2840 - binary_accuracy: 0.8941 - val_loss: 0.2841 - val_binary_accuracy: 0.8941\n",
      "Epoch 39/250\n",
      "2803/2803 [==============================] - 41s 14ms/step - loss: 0.2839 - binary_accuracy: 0.8941 - val_loss: 0.2840 - val_binary_accuracy: 0.8941\n",
      "Epoch 40/250\n",
      "2803/2803 [==============================] - 44s 15ms/step - loss: 0.2838 - binary_accuracy: 0.8941 - val_loss: 0.2840 - val_binary_accuracy: 0.8942\n",
      "Epoch 41/250\n",
      "2803/2803 [==============================] - 37s 13ms/step - loss: 0.2837 - binary_accuracy: 0.8941 - val_loss: 0.2839 - val_binary_accuracy: 0.8942\n",
      "Epoch 42/250\n",
      "2803/2803 [==============================] - 41s 14ms/step - loss: 0.2837 - binary_accuracy: 0.8941 - val_loss: 0.2838 - val_binary_accuracy: 0.8942\n",
      "Epoch 43/250\n",
      "2803/2803 [==============================] - 36s 12ms/step - loss: 0.2836 - binary_accuracy: 0.8941 - val_loss: 0.2837 - val_binary_accuracy: 0.8942\n",
      "Epoch 44/250\n",
      "2803/2803 [==============================] - 44s 15ms/step - loss: 0.2835 - binary_accuracy: 0.8941 - val_loss: 0.2837 - val_binary_accuracy: 0.8942\n",
      "Epoch 45/250\n",
      "2803/2803 [==============================] - 42s 14ms/step - loss: 0.2834 - binary_accuracy: 0.8941 - val_loss: 0.2836 - val_binary_accuracy: 0.8942\n",
      "Epoch 46/250\n",
      "2803/2803 [==============================] - 47s 16ms/step - loss: 0.2834 - binary_accuracy: 0.8941 - val_loss: 0.2836 - val_binary_accuracy: 0.8942\n",
      "Epoch 47/250\n",
      "2803/2803 [==============================] - 42s 14ms/step - loss: 0.2833 - binary_accuracy: 0.8942 - val_loss: 0.2835 - val_binary_accuracy: 0.8942\n",
      "Epoch 48/250\n",
      "2803/2803 [==============================] - 45s 15ms/step - loss: 0.2832 - binary_accuracy: 0.8941 - val_loss: 0.2834 - val_binary_accuracy: 0.8942\n",
      "Epoch 49/250\n",
      "2803/2803 [==============================] - 39s 13ms/step - loss: 0.2832 - binary_accuracy: 0.8941 - val_loss: 0.2834 - val_binary_accuracy: 0.8942\n",
      "Epoch 50/250\n",
      "2803/2803 [==============================] - 45s 15ms/step - loss: 0.2831 - binary_accuracy: 0.8942 - val_loss: 0.2833 - val_binary_accuracy: 0.8942\n",
      "Epoch 51/250\n",
      "2803/2803 [==============================] - 37s 13ms/step - loss: 0.2831 - binary_accuracy: 0.8942 - val_loss: 0.2833 - val_binary_accuracy: 0.8942\n",
      "Epoch 52/250\n",
      "2803/2803 [==============================] - 42s 14ms/step - loss: 0.2830 - binary_accuracy: 0.8941 - val_loss: 0.2832 - val_binary_accuracy: 0.8942\n",
      "Epoch 53/250\n",
      "2803/2803 [==============================] - 45s 15ms/step - loss: 0.2829 - binary_accuracy: 0.8941 - val_loss: 0.2832 - val_binary_accuracy: 0.8942\n",
      "Epoch 54/250\n",
      "2803/2803 [==============================] - 47s 16ms/step - loss: 0.2829 - binary_accuracy: 0.8941 - val_loss: 0.2831 - val_binary_accuracy: 0.8942\n",
      "Epoch 55/250\n",
      "2803/2803 [==============================] - 47s 16ms/step - loss: 0.2828 - binary_accuracy: 0.8942 - val_loss: 0.2831 - val_binary_accuracy: 0.8942\n",
      "Epoch 56/250\n",
      "2803/2803 [==============================] - 45s 16ms/step - loss: 0.2828 - binary_accuracy: 0.8942 - val_loss: 0.2830 - val_binary_accuracy: 0.8942\n",
      "Epoch 57/250\n",
      "2803/2803 [==============================] - 42s 14ms/step - loss: 0.2828 - binary_accuracy: 0.8942 - val_loss: 0.2830 - val_binary_accuracy: 0.8942\n",
      "Epoch 58/250\n",
      "2803/2803 [==============================] - 42s 14ms/step - loss: 0.2827 - binary_accuracy: 0.8942 - val_loss: 0.2830 - val_binary_accuracy: 0.8942\n",
      "Epoch 59/250\n",
      "2803/2803 [==============================] - 43s 15ms/step - loss: 0.2827 - binary_accuracy: 0.8942 - val_loss: 0.2829 - val_binary_accuracy: 0.8942\n",
      "Epoch 60/250\n",
      "2803/2803 [==============================] - 46s 16ms/step - loss: 0.2826 - binary_accuracy: 0.8942 - val_loss: 0.2829 - val_binary_accuracy: 0.8942\n",
      "Epoch 61/250\n",
      "2803/2803 [==============================] - 46s 16ms/step - loss: 0.2826 - binary_accuracy: 0.8942 - val_loss: 0.2829 - val_binary_accuracy: 0.8942\n",
      "Epoch 62/250\n",
      "2803/2803 [==============================] - 46s 16ms/step - loss: 0.2825 - binary_accuracy: 0.8942 - val_loss: 0.2828 - val_binary_accuracy: 0.8942\n",
      "Epoch 63/250\n",
      "2803/2803 [==============================] - 42s 14ms/step - loss: 0.2825 - binary_accuracy: 0.8942 - val_loss: 0.2828 - val_binary_accuracy: 0.8942\n",
      "Epoch 64/250\n",
      "2803/2803 [==============================] - 42s 14ms/step - loss: 0.2825 - binary_accuracy: 0.8942 - val_loss: 0.2828 - val_binary_accuracy: 0.8943\n",
      "Epoch 65/250\n",
      "2803/2803 [==============================] - 41s 14ms/step - loss: 0.2824 - binary_accuracy: 0.8942 - val_loss: 0.2827 - val_binary_accuracy: 0.8943\n",
      "Epoch 66/250\n",
      "2803/2803 [==============================] - 54s 19ms/step - loss: 0.2824 - binary_accuracy: 0.8942 - val_loss: 0.2827 - val_binary_accuracy: 0.8942\n",
      "Epoch 67/250\n",
      "2803/2803 [==============================] - 49s 17ms/step - loss: 0.2824 - binary_accuracy: 0.8942 - val_loss: 0.2827 - val_binary_accuracy: 0.8943\n",
      "Epoch 68/250\n",
      "2803/2803 [==============================] - 47s 16ms/step - loss: 0.2823 - binary_accuracy: 0.8942 - val_loss: 0.2826 - val_binary_accuracy: 0.8943\n",
      "Epoch 69/250\n",
      "2803/2803 [==============================] - 56s 19ms/step - loss: 0.2823 - binary_accuracy: 0.8942 - val_loss: 0.2826 - val_binary_accuracy: 0.8943\n",
      "Epoch 70/250\n",
      "2803/2803 [==============================] - 54s 18ms/step - loss: 0.2823 - binary_accuracy: 0.8942 - val_loss: 0.2826 - val_binary_accuracy: 0.8943\n",
      "Epoch 71/250\n",
      "2803/2803 [==============================] - 59s 20ms/step - loss: 0.2822 - binary_accuracy: 0.8942 - val_loss: 0.2826 - val_binary_accuracy: 0.8943\n",
      "Epoch 72/250\n",
      "2803/2803 [==============================] - 47s 16ms/step - loss: 0.2822 - binary_accuracy: 0.8942 - val_loss: 0.2825 - val_binary_accuracy: 0.8943\n",
      "Epoch 73/250\n",
      "2803/2803 [==============================] - 46s 16ms/step - loss: 0.2822 - binary_accuracy: 0.8942 - val_loss: 0.2825 - val_binary_accuracy: 0.8943\n",
      "Epoch 74/250\n",
      "2803/2803 [==============================] - 47s 16ms/step - loss: 0.2821 - binary_accuracy: 0.8942 - val_loss: 0.2825 - val_binary_accuracy: 0.8944\n",
      "Epoch 75/250\n",
      "2803/2803 [==============================] - 46s 16ms/step - loss: 0.2821 - binary_accuracy: 0.8942 - val_loss: 0.2825 - val_binary_accuracy: 0.8944\n",
      "Epoch 76/250\n",
      "2803/2803 [==============================] - 47s 16ms/step - loss: 0.2821 - binary_accuracy: 0.8942 - val_loss: 0.2824 - val_binary_accuracy: 0.8943\n",
      "Epoch 77/250\n",
      "2803/2803 [==============================] - 47s 16ms/step - loss: 0.2821 - binary_accuracy: 0.8942 - val_loss: 0.2824 - val_binary_accuracy: 0.8943\n",
      "Epoch 78/250\n",
      "2803/2803 [==============================] - 51s 17ms/step - loss: 0.2820 - binary_accuracy: 0.8943 - val_loss: 0.2824 - val_binary_accuracy: 0.8943\n",
      "Epoch 79/250\n",
      "2803/2803 [==============================] - 52s 18ms/step - loss: 0.2820 - binary_accuracy: 0.8942 - val_loss: 0.2824 - val_binary_accuracy: 0.8943\n",
      "Epoch 80/250\n",
      "2803/2803 [==============================] - 48s 16ms/step - loss: 0.2820 - binary_accuracy: 0.8942 - val_loss: 0.2823 - val_binary_accuracy: 0.8943\n",
      "Epoch 81/250\n",
      "2803/2803 [==============================] - 44s 15ms/step - loss: 0.2819 - binary_accuracy: 0.8943 - val_loss: 0.2823 - val_binary_accuracy: 0.8944\n",
      "Epoch 82/250\n",
      "2803/2803 [==============================] - 56s 19ms/step - loss: 0.2819 - binary_accuracy: 0.8942 - val_loss: 0.2823 - val_binary_accuracy: 0.8943\n",
      "Epoch 83/250\n",
      "2803/2803 [==============================] - 55s 19ms/step - loss: 0.2819 - binary_accuracy: 0.8942 - val_loss: 0.2823 - val_binary_accuracy: 0.8943\n",
      "Epoch 84/250\n",
      "2803/2803 [==============================] - 58s 20ms/step - loss: 0.2819 - binary_accuracy: 0.8942 - val_loss: 0.2823 - val_binary_accuracy: 0.8944\n",
      "Epoch 85/250\n",
      "2803/2803 [==============================] - 52s 18ms/step - loss: 0.2818 - binary_accuracy: 0.8943 - val_loss: 0.2823 - val_binary_accuracy: 0.8943\n",
      "Epoch 86/250\n",
      "2803/2803 [==============================] - 48s 17ms/step - loss: 0.2818 - binary_accuracy: 0.8942 - val_loss: 0.2822 - val_binary_accuracy: 0.8943\n",
      "Epoch 87/250\n",
      "2803/2803 [==============================] - 46s 16ms/step - loss: 0.2818 - binary_accuracy: 0.8943 - val_loss: 0.2822 - val_binary_accuracy: 0.8944\n",
      "Epoch 88/250\n",
      "2803/2803 [==============================] - 49s 17ms/step - loss: 0.2818 - binary_accuracy: 0.8943 - val_loss: 0.2822 - val_binary_accuracy: 0.8944\n",
      "Epoch 89/250\n",
      "2803/2803 [==============================] - 60s 21ms/step - loss: 0.2818 - binary_accuracy: 0.8943 - val_loss: 0.2822 - val_binary_accuracy: 0.8944\n",
      "Epoch 90/250\n",
      "2803/2803 [==============================] - 55s 19ms/step - loss: 0.2817 - binary_accuracy: 0.8943 - val_loss: 0.2822 - val_binary_accuracy: 0.8944\n",
      "Epoch 91/250\n",
      "2803/2803 [==============================] - 49s 17ms/step - loss: 0.2817 - binary_accuracy: 0.8943 - val_loss: 0.2821 - val_binary_accuracy: 0.8944\n",
      "Epoch 92/250\n",
      "2803/2803 [==============================] - 51s 18ms/step - loss: 0.2817 - binary_accuracy: 0.8943 - val_loss: 0.2821 - val_binary_accuracy: 0.8944\n",
      "Epoch 93/250\n",
      "2803/2803 [==============================] - 48s 16ms/step - loss: 0.2817 - binary_accuracy: 0.8943 - val_loss: 0.2821 - val_binary_accuracy: 0.8943\n",
      "Epoch 94/250\n",
      "2803/2803 [==============================] - 45s 16ms/step - loss: 0.2816 - binary_accuracy: 0.8943 - val_loss: 0.2821 - val_binary_accuracy: 0.8943\n",
      "Epoch 95/250\n",
      "2803/2803 [==============================] - 47s 16ms/step - loss: 0.2816 - binary_accuracy: 0.8943 - val_loss: 0.2821 - val_binary_accuracy: 0.8944\n",
      "Epoch 96/250\n",
      "2803/2803 [==============================] - 44s 15ms/step - loss: 0.2816 - binary_accuracy: 0.8943 - val_loss: 0.2821 - val_binary_accuracy: 0.8944\n",
      "Epoch 97/250\n",
      "2803/2803 [==============================] - 42s 14ms/step - loss: 0.2816 - binary_accuracy: 0.8943 - val_loss: 0.2820 - val_binary_accuracy: 0.8944\n",
      "Epoch 98/250\n",
      "2803/2803 [==============================] - 39s 13ms/step - loss: 0.2816 - binary_accuracy: 0.8943 - val_loss: 0.2820 - val_binary_accuracy: 0.8944\n",
      "Epoch 99/250\n",
      "2803/2803 [==============================] - 41s 14ms/step - loss: 0.2815 - binary_accuracy: 0.8943 - val_loss: 0.2820 - val_binary_accuracy: 0.8944\n",
      "Epoch 100/250\n",
      "2803/2803 [==============================] - 47s 16ms/step - loss: 0.2815 - binary_accuracy: 0.8943 - val_loss: 0.2820 - val_binary_accuracy: 0.8944\n",
      "Epoch 101/250\n",
      "2803/2803 [==============================] - 56s 19ms/step - loss: 0.2815 - binary_accuracy: 0.8943 - val_loss: 0.2820 - val_binary_accuracy: 0.8944\n",
      "Epoch 102/250\n",
      "2803/2803 [==============================] - 59s 21ms/step - loss: 0.2815 - binary_accuracy: 0.8943 - val_loss: 0.2820 - val_binary_accuracy: 0.8944\n",
      "Epoch 103/250\n",
      "2803/2803 [==============================] - 49s 17ms/step - loss: 0.2815 - binary_accuracy: 0.8943 - val_loss: 0.2820 - val_binary_accuracy: 0.8944\n",
      "Epoch 104/250\n",
      "2803/2803 [==============================] - 52s 18ms/step - loss: 0.2814 - binary_accuracy: 0.8943 - val_loss: 0.2820 - val_binary_accuracy: 0.8944\n",
      "Epoch 105/250\n",
      "2803/2803 [==============================] - 52s 18ms/step - loss: 0.2814 - binary_accuracy: 0.8943 - val_loss: 0.2819 - val_binary_accuracy: 0.8944\n",
      "Epoch 106/250\n",
      "2803/2803 [==============================] - 50s 17ms/step - loss: 0.2814 - binary_accuracy: 0.8943 - val_loss: 0.2819 - val_binary_accuracy: 0.8944\n",
      "Epoch 107/250\n",
      "2803/2803 [==============================] - 49s 17ms/step - loss: 0.2814 - binary_accuracy: 0.8943 - val_loss: 0.2819 - val_binary_accuracy: 0.8944\n",
      "Epoch 108/250\n",
      "2803/2803 [==============================] - 46s 16ms/step - loss: 0.2814 - binary_accuracy: 0.8943 - val_loss: 0.2819 - val_binary_accuracy: 0.8944\n",
      "Epoch 109/250\n",
      "2803/2803 [==============================] - 42s 14ms/step - loss: 0.2813 - binary_accuracy: 0.8943 - val_loss: 0.2819 - val_binary_accuracy: 0.8944\n",
      "Epoch 110/250\n",
      "2803/2803 [==============================] - 40s 14ms/step - loss: 0.2813 - binary_accuracy: 0.8943 - val_loss: 0.2819 - val_binary_accuracy: 0.8944\n",
      "Epoch 111/250\n",
      "2803/2803 [==============================] - 40s 14ms/step - loss: 0.2813 - binary_accuracy: 0.8943 - val_loss: 0.2819 - val_binary_accuracy: 0.8944\n",
      "Epoch 112/250\n",
      "1971/2803 [====================>.........] - ETA: 10s - loss: 0.2810 - binary_accuracy: 0.8944"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4356/2344295452.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Start training..\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\n",
    "train_dataset = dataset.shuffle(len(X_train)).batch(500)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))\n",
    "validation_dataset = dataset.shuffle(len(X_test)).batch(500)\n",
    "\n",
    "print(\"Start training..\\n\")\n",
    "history = model.fit(train_dataset, epochs=250, validation_data=validation_dataset)\n",
    "print(\"Done.\")\n",
    "\n",
    "precision = history.history['precision']\n",
    "val_precision = history.history['val_precision']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(precision) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0c1af1f0b7c0e09eaf62ad50ff8bf7eba640815680725849997a6c6d0940f21"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
